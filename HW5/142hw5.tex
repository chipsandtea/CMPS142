\documentclass[11pt]{article}
\usepackage{geometry} 
\usepackage[utf8]{inputenc}
\geometry{margin=1in} 
\usepackage{fullpage,amsthm,amsfonts,amssymb,epsfig,amsmath,times,amsthm,mathtools,enumitem,amsmath,graphicx}
\usepackage{pgfplots}
\usepackage{diagbox}
\usepackage{tikz-qtree}
\usepackage[retainorgcmds]{IEEEtrantools}


\title{CMPS 142 - Homework 5}
\author{Aidan Gadberry, Christopher Hsiao, Tom Burch\\ \{agadberr, chhsiao, tburch1\}@ucsc.edu}
\date{June 8, 2017} 

\begin{document}
\maketitle

\section{Back Propagation}
\subsection{Feed Foward}
First, we will perform the feed forward process to calculate the output of our neural network: $z_5$.\\
Note that we will be using $\sigma(a_j)$ to represent the logistic sigmoid function on the activations of nodes $j = 1... 5$.\\

\textbf{Input Layer - 1, 2}\\
\begin{center}
Given an input of $(1, 2)$, $a_1 = 1$ and $a_2 = 2$, we can calculate the outputs:
\begin{gather*}
z_1 = \sigma(a_1) = \sigma(1) \approx 0.73 \\
z_2 = \sigma(a_2) = \sigma(2) \approx 0.88
\end{gather*}
\end{center}

\textbf{Hidden Layer - 3, 4}
\begin{center}
First, we'll calculate the activations $a_3, a_4$ for the hidden nodes.
\begin{gather*}
a_3 = z_1 \ w_{3,1}  + z_2 \ w_{3,2} \approx 0.536\\
a_4 = z_1 \ w_{4,1}  + z_2 \ w_{4,2} \approx 0.536\\
\end{gather*}
And we can now calculate the outputs of these nodes:
\begin{gather*}
z_3 = \sigma(a_3) = \sigma(0.536) \approx 0.631\\
z_4 = \sigma(a_4) = \sigma(0.536) \approx 0.631
\end{gather*}
\end{center}

\textbf{Output - 5}
\begin{center}
And now to calculate the output activation $a_5$. Recall that for the output node, $a_5 = z_5$.
\begin{gather*}
a_5 = z_3 \ w_{5,3} + z_4 \ w_{5, 4} = 0\\
\text{thus, } a_5 = z_5 = 0
\end{gather*}
\end{center}

\subsection{Backpropagation}
Now we'll begin calculating the $\frac{\delta E}{\delta a_j}$ values, which we'll represent as $\delta_j$, starting with $\delta_5$.\\

\textbf{Output Node: $\delta_5$}
\begin{center}
Since the error function is:
\[E = \frac{1}{2}(a_5 - t)^2\]
then...
\[\delta_5 = \frac{\delta E}{\delta a_5} = (a_5 -t) = (1-2) = -1\]
\end{center}

\begin{center}
Now, recall that the general equation for other $\delta_j$ values is:
\[\frac{\delta E}{\delta a_j} = \delta_j = \bigg( \sum_{k\in U_j} \delta_k \ w_{k, j}\bigg) \ z_j(1-z_j)\]
\indent Where $U_j$ represented outgoing arcs from node $j$.\\
\end{center}
\textbf{Hidden Layer - 3, 4}
\begin{center}
Using $\delta_5$, we'll use the equation above to calculate $\delta_4 \text{ and } \delta_3$.
\[ \delta_4 = (-1  \cdot 1)(0.631)(0.369) \approx -0.233 \]
\[ \delta_3 = (-1  \cdot -1)(0.631)(0.369) \approx 0.233 \]
\end{center}

\textbf{Input Layer - 1, 2}
\begin{center}
And now to find $\delta_1 \text{ and } \delta_2.$
\[ \delta_2 = [(\delta_3 \cdot \frac{1}{3}) + (\delta_4 \cdot \frac{1}{3})] \ (0.88)(1-0.88) = 0 \]
\[ \delta_1 = [(\delta_3 \cdot \frac{1}{3}) + (\delta_4 \cdot \frac{1}{3})] \ (0.73)(1-0.73) = 0 \]
\end{center}


\textbf{Updating the Weights - }$w_{j,i}$'s\\

Recall that the full step/update equation in SGD for each weight is:
\[ w_{j,i} \coloneqq w_{j,i}  -  \eta \frac{\delta E}{\delta w_{j,i}} \]
\[\text{where: } \frac{\delta E}{\delta w_{j,i}} = \delta_j \ z_i \]
\pagebreak

Thus, the updates for the weights are as follows:
\begin{center}
\begin{gather*}
w_{5, 3} = -1 - (0.1)(-1)(0.631) = -0.937\\
w_{5, 4} = 1 - (0.1)(-1)(0.631) = 1.063\\
w_{4, 1} = \frac{1}{3} - (0.1)(-0.233)(0.73) = 0.35\\
w_{4, 2} = \frac{1}{3} - (0.1)(-0.233)(0.88) = 0.354\\
w_{3, 1} = \frac{1}{3} - (0.1)(0.233)(0.73) = 0.316\\
w_{3, 2} = \frac{1}{3} - (0.1)(0.233)(0.88) = 0.313
\end{gather*}
\end{center}



\section{Clustering}
Given: {1, 3, 6, 10, 15, 21, 28, 36}, K=3, Given mean={1},{3},{6}\\

\begin {tabular}{c|c c|c c|c c}
Iteration & K1 & K1-mean & K2 & K2-mean & K3 & K3-mean\\
\hline
\#1 & {1} & {1} & {3} & {3} & {6, 10, 15, 21, 28, 38} & {19.3} \\
\hline
\#2 & {1,3} & {1.5} & {6} & {6} & {10, 15, 21, 28, 36} & {22} \\
\hline
\#3 & {1,3,6} & {3.3} & {10} & {10} & {15, 21, 28, 36} & {25} \\
\hline
\#4 & {1,3,6,10} & {5} & {15} & {15} & {21, 28, 36} & {28.3} \\
\hline
\#5 & {1,3,6,10,15} & {7} & {21} & {21} & {28, 38} & {32} \\
\hline
\#6 & {1,3,6,10,15,21} & {9.3} & {28} & {28} & {36} & {36} \\
\end{tabular}
\newline

Given: {1, 3, 6, 10, 15, 21, 28, 36}, K=3, Given mean={21},{28},{36} \\

\begin {tabular}{c|c c|c c|c c}
Iteration & K1 & K1-mean & K2 & K2-mean & K3 & K3-mean\\
\hline
\#1 & {1,3,6,10,15,21} & {9.3} & {28} & {28} & {36} & {36} \\
\end{tabular}
\newline

Yes, we believe starting with the same given set {1, 3, 6, 10, 15, 21, 28, 36}, the K-cluster algorithm will output the same clustering when comparing the initial means [{1},{3},{6}] and [{21},{28},{36}]. Although the final clustering is identical, the difference is the execution of the K-means cluster algorithm, where for the first set of initial means will result in the algorithm processing 6 iterations, while the the second set of means will allow the algorithm to complete in one iteration.

\end{document}