\documentclass[11pt]{article}
\usepackage{fullpage,amsthm,amsfonts,amssymb,epsfig,amsmath,times,amsthm,mathtools,enumitem,amsmath,graphicx}

\newtheorem{theorem}{Theorem}
\newtheorem{claim}[theorem]{Claim}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\ceil}[1]{\lceil #1 \rceil}
\newcommand\tab[1][1cm]{\hspace*{#1}}
\newcommand{\abs}[1]{\mid#1\mid}
\begin{document}

\begin{center}
{\bf\Large CMPS142 Homework 2}
\end{center}
\noindent $\mathbf{Connor\;Pryor}$\\
\noindent $\mathbf{Student\;ID\;1372839}$\\
\\
\noindent $\mathbf{Christopher\;Hsiao}$\\
\noindent $\mathbf{Student\;ID\;1398305}$\\

\section*{1. Naive Bayes}
Use maximum likelihood estimation (not the unbiased or Laplace estimates) for the distributions of the two features conditioned on the two classes in the figure below. Give the mean and variance of the gaussians you found for the GPA.\newline
\begin{center}
\begin{tabular}{ |c|c|c|c| } 
\hline
label & AP & GPA \\
\hline

H & yes & 4.0 \\ 
H & yes & 3.7 \\ 
H & no & 2.5 \\ 
N & no & 3.8 \\ 
N & yes & 3.3 \\ 
N & yes & 3.0 \\ 
N & no & 3.0 \\ 
N & no & 2.7 \\ 
N & no & 2.2 \\
\hline
\end{tabular}
\end{center}

\noindent \textbf{Mean and Variance}\newline

\noindent Honor Students:\newline

$mean = \dfrac{4.0+3.7+2.5}{3}$\newline

$\hphantom{mean }= 3.4$\newline

$variance = \dfrac{(4.0-3.4)^2 + (3.7-3.4)^2 + (2.5-3.4)^2}{3} $\newline

$\hphantom{variance }= 0.42$\newline

\noindent Normal Students:\newline

$mean = \dfrac{3.8+3.3+3.0+3.0+2.7+2.2}{6}$\newline

$\hphantom{mean }= 3.0$\newline

$variance = \dfrac{(3.8-3.0)^2 + (3.3-3.0)^2 + (3.0-3.0)^2 + (3.0-3.0)^2 + (2.7-3.0)^2 + (2.2-3.0)^2}{6}$\newline

$\hphantom{variance }= 0.243$\newline

\noindent \textbf{Maximum Likelihood Estimation}\newline

\noindent One dimensional Gausians: \newline



\begin{center}
$p(x) = \dfrac{1}{\sqrt{2\pi}\sigma}e^{-\dfrac{(x-\mu)^2}{2\sigma^2}}$
\end{center}

\noindent Probability of AP: \newline

\textit{Given H:}

\begin{center}
$P(AP|H) = \dfrac{P(AP \kern .1cm and \kern .1cm H)}{P(H)} = \dfrac{\frac{2}{9}}{\frac{3}{9}} = \dfrac{2}{3}$
\end{center}

\textit{Given N:}

\begin{center}
$P(AP|N) = \dfrac{P(AP \kern .1cm and \kern .1cm N)}{P(N)} = \dfrac{\frac{2}{9}}{\frac{6}{9}} = \dfrac{1}{3}$
\end{center}

\noindent Probability of $\neg$ AP: \newline

\textit{Given H:}

\begin{center}
$P(\neg AP|H) = \dfrac{P(\neg AP \kern .1cm and \kern .1cm H)}{P(H)} = \dfrac{\frac{1}{9}}{\frac{3}{9}} = \dfrac{1}{3}$
\end{center}

\textit{Given N:}

\begin{center}
$P(\neg AP|N) = \dfrac{P(\neg AP \kern .1cm and \kern .1cm N)}{P(N)} = \dfrac{\frac{4}{9}}{\frac{6}{9}} = \dfrac{2}{3}$
\end{center}

\noindent Predict Honor or Normal given AP and GPA:\newline

To predict if a student is a Honor or Normal student given AP and a Gausian distribution of the GPA. The decision boundary for Honor and Normal will be the same. This means the probability to be a Honor student given they have taken AP classes and a curtain GPA will be equal to the probability to be a Normal student given they have taken AP classes and have the same GPA. Otherwise known as:

\begin{center}
$P(H|GPA,AP) = P(N|GPA,AP)$
\end{center}

\newpage
By Bayes Rule:

\begin{center}
$P(H|GPA,AP) = \dfrac{P(GPA,AP|H)P(H)}{P(GPA,AP|H)P(H)+P(GPA,AP|N)P(N)}$

$P(N|GPA,AP) = \dfrac{P(GPA,AP|N)P(N)}{P(GPA,AP|H)P(H)+P(GPA,AP|N)P(N)}$
\end{center}

Since GPA and AP are independent, this can be rewritten as:

\begin{center}
$P(H|GPA,AP) = \dfrac{P(GPA|H)P(AP|H)P(H)}{P(GPA|H)P(AP|H)P(H)+P(GPA|N)P(AP|N)P(N)}$

$P(N|GPA,AP) = \dfrac{P(GPA|N)P(AP|N)P(N)}{P(GPA|H)P(AP|H)P(H)+P(GPA|N)P(AP|N)P(N)}$
\end{center}

Since $P(H|GPA,AP) = P(N|GPA,AP)$ then:

\begin{center}
$P(GPA|H)P(AP|H)P(H) = P(GPA|N)P(AP|N)P(N)$
\end{center}

Moving around the equation and sbstituting in $P(H)$, $P(N)$, $P(AP|H)$, and $P(AP|N)$

\section*{2. Gaussian Discriminant Analysis}
\noindent \textit{a.)} Show that the set of points \textbf{x} such that $p(Y = 0 | x) = p(Y = 1 | x )$ is described by $\textbf{w $\cdot$ x} = b$ for some $\textbf{w}$ and $b$.\\ \\
We can assume that the label probabilities for $Y = 0 \text{ and } Y = 1$ are the same. Which is to say
\begin{equation}p(Y = 0) = p(Y = 1) = \frac{1}{2}\end{equation}
Using Bayes Theorem, the conditional probabilities given \textbf{x} are as follows:
\begin{equation}p(x | Y = 0) = \frac{(Y = 0 | x)p(x)}{p(Y = 0)} \text{ ;   } p(x | Y = 1) = \frac{(Y = 0 | x)p(x)}{p(Y = 1)}\end{equation}
Thus, we can deduce from $(1)$ and $(2)$ that:
\begin{equation}p(Y = 0) = \frac{(Y = 0 | x)p(x)}{p(x | Y = 0)} = \frac{(Y = 1 | x)p(x)}{p(x | Y = 1)} = p(Y=1)\end{equation}
We can deduce from this that:
\begin{equation}p(Y = 1 | x) \cdot p(x | Y=0) = p(Y = 0 | x) \cdot p(x | Y=1)\end{equation}
\\
We are attempting to classify values of $x$ such that $p(Y = 0 | x) = p(Y = 1 | x)$, so we'll assume that this statement is true.
Since these probabilities are equal, we have 
\begin{equation}p(Y = 0 | x) = 1 - p(Y = 1 | x)\end{equation}
Let $P = p(Y = 0|x)$. Then from (5), it follows that $P = 1 - P = p(Y = 1 | x)$.\\
To sum up thus far, we have:
\begin{equation}P \cdot p(x | Y = 0) =  P \cdot (x | Y = 1)\end{equation}

Returning to our original models since, we can now see that
\begin{equation}
\frac{1}{(2\pi) ^ \frac{n}{2}|\sum| ^ \frac{1}{2}} \cdot \exp{(-\frac{1}{2}((x - \mu_0)^T\Sigma^{-1}(x-\mu_0)))} = \frac{1}{(2\pi) ^ \frac{n}{2}|\sum| ^ \frac{1}{2}} \cdot \exp{(-\frac{1}{2}((x - \mu_1)^T\Sigma^{-1}(x-\mu_1)))}
\end{equation}

Which we can clearly cancel out the like terms on either side, and we have the equation
\begin{equation}
\exp{-\frac{1}{2}((x - \mu_0)^T\Sigma^{-1}(x-\mu_0))} = \exp{-\frac{1}{2}((x - \mu_1)^T\Sigma^{-1}(x-\mu_1))}
\end{equation}
We can eliminate the $\exp$ and $-\frac{1}{2}$ by taking the log of both sides of the equation, then cancelling out $-\frac{1}{2}$.\\
Now, the equation we're solving becomes:
\begin{equation}
(x-\mu_0)^T\Sigma^{-1}(x-\mu_0) = (x-\mu_1)^T\Sigma^{-1}(x-\mu_1)
\end{equation}
By distributing the $T$, 
\begin{equation}
(x^T-\mu^T_0)\Sigma^{-1}(x-\mu_0) = (x^T-\mu^T_1)\Sigma^{-1}(x-\mu_1)
\end{equation}
Now, we can multiply $\Sigma^{-1}(x-\mu_0) \text{ by } (x^T-\mu^T_0)$
\begin{equation}
x^T\Sigma^{-1}(x-\mu_0)-\mu^T_0\Sigma^{-1}(x-\mu_0) = x^T\Sigma^{-1}(x-\mu_1)-\mu^T_1\Sigma^{-1}(x-\mu_1)
\end{equation}
Distributing all terms further, we arrive at
\begin{equation}
x^T\Sigma^{-1}x -x^T\Sigma^{-1}\mu_0 - \mu^T_0\Sigma^{-1}x + \mu^T_0\Sigma^{-1}\mu_0 = x^T\Sigma^{-1}x -x^T\Sigma^{-1}\mu_1 - \mu^T_1\Sigma^{-1}x + \mu^T_1\Sigma^{-1}\mu_1
\end{equation}
To reduce from here, we use the property of taking the transpose of a product (equation 18, below) to find that:
\begin{equation}
-2\mu^T_0\Sigma^{-1}x + \mu^T_0\Sigma^{-1}\mu_0 = -2\mu^T_1\Sigma^{-1}x + \mu^T_1\Sigma^{-1}\mu_1
\end{equation}
We group $-2\mu^T_0\Sigma^{-1}x \text{ and } -2\mu^T_1\Sigma^{-1}x$ and reduce to get
\begin{equation}
2(\mu^T_0\Sigma^{-1} - \mu^T_1\Sigma^{-1}) \cdot x = \mu^T_1\Sigma^{-1}\mu_1 - \mu^T_0\Sigma^{-1}\mu_0
\end{equation}
And by simply dividing the equation by $2$, we end up with
\begin{equation}
(\mu^T_0\Sigma^{-1} - \mu^T_1\Sigma^{-1}) \cdot x = \frac{1}{2}(\mu^T_1\Sigma^{-1}\mu_1 - \mu^T_0\Sigma^{-1}\mu_0)
\end{equation}


\noindent Observe that $\mu^T_0\Sigma^{-1} - \mu^T_1\Sigma^{-1}$ is a vector, as $\mu_0$ and $\mu_1$ are vectors. We will call this vector $w$.\\

\noindent Observe further that $\mu^T_1\Sigma^{-1}\mu_1 - \mu^T_0\Sigma^{-1}\mu_0$ is a scalar, because the product of a vector and its transpose yields a scalar. We will call this scalar $b$.\\

\noindent We now have the equation
\begin{equation}
w \cdot x = b
\end{equation}
Recall that the original assumption made was that $p(Y=0|x) = p(Y=1|x)$. As such, this assumption implies that there exists some vector $w$ and some scalar $b$ such that $w \cdot x = b$ lies on the hyperplane.

\subsection*{Property of Taking the Transpose of a Product}
Take note of the following property of taking the transpose of a product, that allows us to say:
\begin{equation}
(x^T\Sigma^{-1}\mu_i)^T = \mu^T_i \Sigma^{-1}x
\end{equation}
Since $\Sigma^{-1}$ is symmetric. Since we know that the product on the left side of (9) itself is a scalar, we can then also drop the transpose, where the equation then becomes:
\begin{equation}
x^T\Sigma^{-1}\mu_i = \mu^T_i \Sigma^{-1}x
\end{equation}

\noindent \textit{b.)} If $p(Y = 0) \neq p(Y = 1)$, then the reduction performed in part \textit{(a.)} would not hold, as it could only be performed when we made the assumption that $p(Y = 0) = p(Y = 1)$. Thus, the decision boundary would no longer be a hyperplane.

\end{document}